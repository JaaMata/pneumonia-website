{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP0CZnQglSI_"
      },
      "source": [
        "# Pneumonia Classification\n",
        "\n",
        "Pneuomia has a mortality rate between 5% to 10%, making the infection extremely dangerous especially in poorer nations which have limited health care infrastructure.\n",
        "\n",
        "Creating a machine learning model will allow for quick identification for pneumonia which will allow for treatment to be administered quickly to save lifes.\n",
        "\n",
        "Due to the nature of the problem CNN seem to be the most effective method to classifiy the data.\n",
        "\n",
        "## Basic Problem Analysis\n",
        "This is a supervised problem with two classes pneumonia or healthy.\n",
        "\n",
        "The data is in the form of images which will be fed into the model which will return the probability of the classes\n",
        "\n",
        "Dataset: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OISnnB0llLT2"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download paultimothymooney/chest-xray-pneumonia\n",
        "! unzip chest-xray-pneumonia.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RRGGda-WlmmS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EdCFKFkJr1Qr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c1c2ad-ffe9-4ba7-dd8b-d63d60c94c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UFEoX5bSwfRQ"
      },
      "outputs": [],
      "source": [
        "rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xVcfJqlAr2UF"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (150, 150)\n",
        "train_dir = '/content/chest_xray/train'\n",
        "valid_dir = '/content/chest_xray/val'\n",
        "test_dir = '/content/chest_xray/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n",
        "\n",
        "Due to the small dataset augmenting the data will be necessary to avoid overfitting.\n",
        "\n",
        "The transformations decided on are:\n",
        "* Rescale to between 0, 1\n",
        "* Shear\n",
        "* Zoom\n",
        "* Width shift\n",
        "* Height shift\n",
        "* Horizontal flip\n",
        "* Fill"
      ],
      "metadata": {
        "id": "kdOwV-pEjM2i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CP__AS0QuPl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6a1536c-7065-4acc-816e-aae3db426730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range = 40,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   fill_mode = 'nearest')\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(train_dir, class_mode='binary', batch_size=BATCH_SIZE, shuffle=True, target_size=IMAGE_SIZE)\n",
        "valid_data = datagen.flow_from_directory(valid_dir, class_mode='binary', batch_size=BATCH_SIZE, shuffle=True, target_size=IMAGE_SIZE)\n",
        "test_data = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=BATCH_SIZE, target_size=IMAGE_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2Vs9Crcpv4ba"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "BY0zN5uuolVi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks\n",
        "\n",
        "Using Early Stopping and ModelCheck point allow to get the best weights of the model and stop overfitting from affecting the models proformace.\n",
        "\n",
        "Using Tensorboard for easy visualistaion of the models scores, outputs."
      ],
      "metadata": {
        "id": "4xehE9TFjw-1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8sp1t9wnwSmw"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 5),\n",
        "                 ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', save_best_only = True), tensorboard_callback]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "TuMICUaSrpJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = Sequential([\n",
        "\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Conv2D(256, 3, padding='same', activation='relu'),\n",
        "    Conv2D(256, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Conv2D(512, 3, padding='same', activation='relu'),\n",
        "    Conv2D(512, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_1.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss=BinaryCrossentropy(),\n",
        "    metrics=['accuracy', recall_m, precision_m, f1_m]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZZttm0uVkMdD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.fit(train_data, epochs=25, validation_data=test_data, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "pABOLGZlkq6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.evaluate(test_data)"
      ],
      "metadata": {
        "id": "qoKW8T5hlavW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "_5wXdNKqnuLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqSql_v6wsEr"
      },
      "outputs": [],
      "source": [
        "model_2 = Sequential([\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPool2D((2, 2)),\n",
        "    \n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPool2D((2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_2.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss=BinaryCrossentropy(),\n",
        "    metrics=['accuracy', recall_m, precision_m, f1_m]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.fit(train_data, epochs=25, validation_data=test_data, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "9Z_kN69ynwCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "HBDVL7W6nwCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "XIJ_iG3anwCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = Sequential([\n",
        "    Conv2D(16, 3, padding='same', input_shape=(150, 150, 3), activation='relu'),\n",
        "    Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    Conv2D(128, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "    Dropout(.2),\n",
        "\n",
        "    Conv2D(256, 3, padding='same', activation='relu'),\n",
        "    Conv2D(256, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "    Dropout(.2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_3.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss=BinaryCrossentropy(),\n",
        "    metrics=['accuracy', recall_m, precision_m, f1_m]\n",
        ")\n"
      ],
      "metadata": {
        "id": "7ucNFzyznkGj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.fit(train_data, epochs=25, validation_data=test_data, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "UPEefR_fnxwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.evaluate(test_data)"
      ],
      "metadata": {
        "id": "2-XdtNwynxwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "6FIbTeymnxwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "The CNN models are not getting an acceptable accuracy score.\n",
        "\n",
        "A transfer learning model has the potential to get a higher metrics.\n",
        "\n",
        "The VGG16 model trained on the imagenet will be used as it was used to classify 1000 classes with a high accuracy."
      ],
      "metadata": {
        "id": "VG4_Ox1jmiHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "base  = VGG16(weights='imagenet',  include_top=False, input_shape=(150, 150, 3))\n",
        "base.trainable = False"
      ],
      "metadata": {
        "id": "uu3snf3kUET4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = Sequential([\n",
        "    base,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_4.compile(\n",
        "        optimizer=Adam(),\n",
        "    loss=BinaryCrossentropy(),\n",
        "    metrics=['accuracy', recall_m, precision_m, f1_m]\n",
        ")"
      ],
      "metadata": {
        "id": "CXQiWd_JUqeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.fit(train_data, epochs=25, validation_data=test_data, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "tJWuimRWVCB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion \n",
        "\n",
        "The transfer learning model got the best accuracy of around 90%, which is not good enough when the decision has a impact on someones medical treatment. Therefore before the model could be pretential used in the real world the accuracy needs to improve.\n",
        "\n",
        "## Improvements\n",
        "\n",
        "The models are held back by the small dataset, so the most effective method to achieve better model metrics would be to gather more data.\n",
        "\n",
        "The models would also improve from finetuning methods such as finding the ideal learning rate.\n"
      ],
      "metadata": {
        "id": "kz8CHwTFqj9W"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}